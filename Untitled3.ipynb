{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96edd90e-7fcc-4cc0-922a-59eaf1d56747",
   "metadata": {},
   "source": [
    "# Loan Default Prediction — Reproducible Project\n",
    "\n",
    "This project contains:\n",
    "- EDA, data cleaning, feature engineering\n",
    "- Classical ML: XGBoost\n",
    "- Deep Learning: Keras ANN and PyTorch MLP\n",
    "- Scripts / Jupyter Notebook cells and saved artifacts\n",
    "\n",
    "## File list\n",
    "- `Loan_Project_Consolidated.ipynb` — consolidated notebook (or paste cells into a new notebook)\n",
    "- `preprocessor.joblib` — saved preprocessing pipeline (created after running notebook)\n",
    "- `xgb_model.json`, `keras_ann.h5`, `pytorch_mlp.pt` — saved models (created after running)\n",
    "- `requirements.txt` — Python package list\n",
    "- `FINAL_REPORT.pdf` — concise report (2-3 pages) summarizing results (you can create from provided text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dc99d8-8987-4d29-98f2-cc85e24b14ea",
   "metadata": {},
   "source": [
    "and run cells in order.\n",
    "\n",
    "## Reproducibility notes\n",
    "- Random seed defined as `RANDOM_SEED = 42`. Results may vary slightly by hardware (GPU/CPU), package versions.\n",
    "- Preprocessing pipeline saved to `preprocessor.joblib`. Use the same preprocessor at inference.\n",
    "- For offline-RL evaluation, the notebook uses a naive policy-value estimator (average realized reward). For robust evaluation, use IPS/DR estimators.\n",
    "\n",
    "## What to inspect\n",
    "XGBoost metrics and feature importance.\n",
    "ANN training curves & test AUC.\n",
    "\n",
    "\n",
    "\n",
    "## Notes / Next steps\n",
    "- Consider temporal splits (train on older loans, test on newer loans) to avoid data leakage.\n",
    "- Consider using `d3rlpy` for a formal offline RL algorithm (CQL/AWAC) and proper off-policy evaluation tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dafa1e-a45b-434d-83e6-a040c948d1d1",
   "metadata": {},
   "source": [
    "## Requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f128bee-762e-48f4-9995-2932b9b09c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy>=1.21\n",
    "# pandas>=1.3\n",
    "# scikit-learn>=1.0\n",
    "# matplotlib>=3.4\n",
    "# seaborn>=0.11\n",
    "# xgboost>=1.5\n",
    "# tensorflow>=2.9\n",
    "# torch>=1.12\n",
    "# joblib>=1.1\n",
    "# nbformat>=5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bbd72c-41e1-46d9-8b3c-1c9f6354d4e4",
   "metadata": {},
   "source": [
    "Loan Default Prediction — Final Report\n",
    "=====================================\n",
    "\n",
    "Author: Pranay Kumbhare\n",
    "Date: 16 sept 2025\n",
    "\n",
    "1. Project summary\n",
    "------------------\n",
    "This project analyzes a loan dataset to predict loan default (binary: Fully Paid = 0, Default/Charged Off = 1) and to learn an offline policy (approve/deny) using a contextual-bandit style reward model. We compare a classical ML model (XGBoost) and two deep learning approaches (Keras ANN and PyTorch MLP). \n",
    "\n",
    "2. Data & preprocessing\n",
    "-----------------------\n",
    "Dataset: (user-provided CSV)\n",
    "\n",
    "Key preprocessing steps:\n",
    "- Normalized column names (lowercase, underscores).\n",
    "- Parsed common fields: `int_rate` normalized from '13.56%' to 0.1356; `term` parsed to numeric months; `emp_length` parsed to years; date fields parsed to datetime; derived `credit_length_years`.\n",
    "- Mapped `loan_status` to `target_default` using domain rules: 'Fully Paid' -> 0, 'Charged Off'/'Default'/'Late (120)' -> 1. Rows without clearly-defined outcome (e.g., current loans) were dropped to avoid label noise.\n",
    "- Dropped columns with >75% missingness (adjustable threshold) to reduce noise.\n",
    "- Built a reproducible `preprocessor` pipeline (ColumnTransformer) that:\n",
    "  - median-imputes numeric data, winsorizes (1–99 percentile), and standard-scales.\n",
    "  - imputes categorical variables with 'missing' and one-hot encodes them.\n",
    "- Produced stratified train/test split to preserve class balance.\n",
    "\n",
    "Rationale:\n",
    "This pipeline is conservative and reproduces transformations in test/inference. Winsorizing limits extreme outliers that skew models; imputation choices are simple but robust. Temporal split is recommended for deployment but stratified split was used for model development.\n",
    "\n",
    "3. Models & training\n",
    "--------------------\n",
    "a) XGBoost (classical baseline)\n",
    "- Hyperparameters: n_estimators=300, lr=0.05, max_depth=6, subsample=0.8, colsample_bytree=0.8.\n",
    "- Training on preprocessed features.\n",
    "- Metrics reported: ROC AUC, F1, precision/recall, and confusion matrix.\n",
    "- Feature importance extracted (if preprocessor supports feature names).\n",
    "\n",
    "b) Keras ANN (deep learning)\n",
    "- Architecture: Dense(128)->Dropout->Dense(64)->Dropout->Dense(1, sigmoid)\n",
    "- Loss: binary_crossentropy; optimizer: Adam; metrics: accuracy & AUC.\n",
    "- Trained with 15 epochs and early validation split.\n",
    "\n",
    "c) PyTorch MLP (alternate deep model)\n",
    "- Architecture similar to Keras model; trained with BCEWithLogitsLoss and Adam.\n",
    "\n",
    "4. Results (example)\n",
    "--------------------\n",
    "(Replace with your dataset-specific numbers after running the notebook.)\n",
    "\n",
    "- XGBoost: AUC = 0.78, F1 = 0.45\n",
    "- Keras ANN: Test AUC = 0.76, Test accuracy = 0.88\n",
    "- PyTorch MLP: AUC = 0.75\n",
    "- RL policy estimated value (avg reward per applicant): e.g., 120.5 (compare with always-approve and always-deny baselines)\n",
    "\n",
    "Interpretation:\n",
    "- AUC/F1 for classifiers capture ranking ability and balance of precision/recall. AUC is threshold-agnostic and useful for model selection; F1 summarizes performance at the chosen threshold and is sensitive to class imbalance.\n",
    "\n",
    "\n",
    "5. Comparison and disagreement analysis\n",
    "--------------------------------------\n",
    "- DL classifier gives a score for default probability; a business rule (threshold) converts it to an approve/deny decision.\n",
    "\n",
    "\n",
    "6. Limitations & future steps\n",
    "-----------------------------\n",
    "- Off-policy evaluation used is naive (direct averaging). For trustworthy deployment evaluate policies using IPS / Doubly Robust estimators or conduct randomized A/B testing.\n",
    "- Reward engineering here is simplified: it ignores time value of money, recovery rates, and collections costs. A better reward should discount future payments, include expected recovery fractions, and account for operational costs.\n",
    "- The dataset may have leakage if features include post-funding events; we printed potential leakage columns for review. For production, remove any feature that wouldn't be available at decision time.\n",
    "- Consider temporal splitting (train on loans issued before year N, test on later loans) to simulate live deployment.\n",
    "\n",
    "\n",
    "7. Reproducibility\n",
    "------------------\n",
    "- Preprocessing pipeline saved as `preprocessor.joblib`.\n",
    "- Models saved as `xgb_model.json`, `keras_ann.h5`, and `pytorch_mlp.pt`.\n",
    "- Use `requirements.txt` to recreate environment.\n",
    "\n",
    "Appendix: Code pointers\n",
    "- See `Loan_Project_Consolidated.ipynb` cells for exact code used for preprocessing, modeling, policy derivation, and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7548a571-bdb4-4d00-9edb-abac25371511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3effa5d-64cf-4e32-9688-eb833bb92214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x16d865f70>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 1: Title + imports\n",
    "# Loan Default Prediction: EDA, ML, DL & Offline RL (contextual-bandit)\n",
    "import os, sys, math, re, json\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# ML & preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# XGBoost (classical ML)\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Keras (DL)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# PyTorch (alternate DL)\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import sklearn\n",
    "\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ada7971d-143a-4aee-9631-292367013a5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccepted_2007_to_2018Q4.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m         nrows\n\u001b[1;32m   1925\u001b[0m     )\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:239\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread(nrows)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_chunk:\n",
      "File \u001b[0;32mpandas/_libs/parsers.pyx:820\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/parsers.pyx:914\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/parsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/parsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen codecs>:331\u001b[0m, in \u001b[0;36mgetstate\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"accepted_2007_to_2018Q4.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0e5ca7-555e-4d2c-9823-e49a96053f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns:\", df.columns.tolist()[:50])\n",
    "print(df.dtypes.value_counts())\n",
    "print(\"\\nTop missing fractions:\")\n",
    "print(df.isnull().mean().sort_values(ascending=False).head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55560fa-71e7-4938-bdeb-58b54828c884",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'loan_status' in df.columns:\n",
    "    print(\"\\nloan_status values sample:\")\n",
    "    print(df['loan_status'].value_counts().head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f64c18-5fcd-4664-b7ff-d1a1ec971a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_candidates = ['loan_amnt','int_rate','annual_inc','dti']  # adjust to your dataset columns\n",
    "num_present = [c for c in num_candidates if c in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822234db-5f80-4f10-83ab-6d1f21587e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4*len(num_present),4))\n",
    "for i,c in enumerate(num_present):\n",
    "    plt.subplot(1,len(num_present),i+1)\n",
    "    sns.histplot(df[c].dropna(), kde=False, bins=40)\n",
    "    plt.title(c)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f5ea09-d5a6-496b-9ee2-782cae0f3cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_candidates = ['home_ownership','verification_status','purpose']\n",
    "cat_present = [c for c in cat_candidates if c in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72896c61-3c73-47ce-8e31-626c0bae262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in cat_present:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.countplot(y=c, data=df, order=df[c].value_counts().index)\n",
    "    plt.title(c)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd51573-ee35-4c21-ae7a-0ba04b7129ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57cb0b9-8091-4fe7-bfa8-813616e79745",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.astype(str).str.strip().str.lower().str.replace(' ', '_').str.replace('-', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50496ce8-7e61-49e2-a0fa-cf349ec6c64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pct(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    s = str(x).strip()\n",
    "    if s.endswith('%'):\n",
    "        try: return float(s.rstrip('%'))/100.0\n",
    "        except: return np.nan\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08a1766-268d-45ab-88e0-eaf0e068b5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emp_len_to_int(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    s = str(x).lower().strip()\n",
    "    if s=='n/a': return np.nan\n",
    "    if '<' in s: return 0.0\n",
    "    if '10+' in s: return 10.0\n",
    "    m = re.search(r'(\\d+)', s)\n",
    "    return float(m.group(1)) if m else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119f81ec-249a-4842-b0c7-e0a63c21c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'int_rate' in df.columns: df['int_rate'] = df['int_rate'].apply(parse_pct)\n",
    "if 'term' in df.columns: df['term'] = df['term'].astype(str).str.extract(r'(\\d+)').astype(float)\n",
    "if 'emp_length' in df.columns: df['emp_length'] = df['emp_length'].apply(emp_len_to_int)\n",
    "for c in ['issue_d','earliest_cr_line','last_pymnt_d','last_credit_pull_d']:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_datetime(df[c], errors='coerce', infer_datetime_format=True)\n",
    "if 'issue_d' in df.columns and 'earliest_cr_line' in df.columns:\n",
    "    df['credit_length_years'] = (df['issue_d'] - df['earliest_cr_line']).dt.days / 365.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01b51bc-1326-4c67-897e-dd1a8bd3325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_target(status):\n",
    "    s = str(status).lower()\n",
    "    if 'fully paid' in s: return 0\n",
    "    if 'charged off' in s or 'default' in s: return 1\n",
    "    if 'late' in s and '120' in s: return 1\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96523f87-1e73-404d-bf1c-3f5140bc8a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'loan_status' not in df.columns:\n",
    "    raise ValueError(\"Expected column 'loan_status' but not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32068336-7073-45b1-ba2f-19c5b21e2d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target_default'] = df['loan_status'].apply(map_target)\n",
    "print(\"target counts (with nulls):\", df['target_default'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f079d2b-f20b-4e30-a91a-0344c8e32c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['target_default'].notna()].copy()\n",
    "df['target_default'] = df['target_default'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b065a8-6f47-479a-957a-061c37ed47da",
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_frac = df.isnull().mean()\n",
    "sparse_cols = miss_frac[miss_frac>0.75].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e64ddf9-aa16-4c99-9a4b-ec1dda0360ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dropping\", len(sparse_cols), \"sparse columns\")\n",
    "df.drop(columns=sparse_cols, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ba7c3e-eda1-403a-9feb-eda76e7d0031",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = set(['loan_status','target_default'])\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.difference(exclude).tolist()\n",
    "cat_cols = df.select_dtypes(include=['object','category']).columns.difference(exclude).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5245169f-35ee-4046-ba85-981f3fee63b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['id','member_id','url','zip_code','addr_state']: \n",
    "    if c in num_cols: num_cols.remove(c)\n",
    "    if c in cat_cols: cat_cols.remove(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03c2612-a2eb-4f62-9819-fc89538bf2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"num_cols:\", len(num_cols), \"cat_cols:\", len(cat_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4d5e9f-348f-48e6-92ce-6d0dfaa26cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def winsorize_array(X):\n",
    "#     X = np.array(X, dtype=float)\n",
    "#     lower = np.nanpercentile(X, 1, axis=0)\n",
    "#     upper = np.nanpercentile(X, 99, axis=0)\n",
    "#     return np.minimum(np.maximum(X, lower), upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333e6cfa-7c81-42a6-b866-c7b8fab7b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f1d880-1a2c-441c-a08d-d78cea78da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# num_transformer = Pipeline([\n",
    "#     ('imputer', SimpleImputer(strategy='median')),\n",
    "#     ('winsor', FunctionTransformer(winsorize_array, validate=False)),\n",
    "#     ('scaler', StandardScaler())\n",
    "# ])\n",
    "\n",
    "# ohe_params = {'handle_unknown': 'ignore'}\n",
    "\n",
    "# if sklearn.__version__ >= \"1.2\":\n",
    "#     ohe_params['sparse_output'] = False  \n",
    "# else:\n",
    "#     ohe_params['sparse'] = False          \n",
    "# cat_transformer = Pipeline([\n",
    "#     ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "#     ('onehot', OneHotEncoder(**ohe_params))\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0553d7-8e64-46fa-85bc-10d25d6af48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor = ColumnTransformer([\n",
    "#     ('num', num_transformer, num_cols),\n",
    "#     ('cat', cat_transformer, cat_cols)\n",
    "# ], remainder='drop', verbose_feature_names_out=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69053a8-0f23-4384-b130-5bfb0713d9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rows:\", df.shape[0])\n",
    "print(\"Num cols:\", len(num_cols))\n",
    "print(\"Cat cols:\", len(cat_cols))\n",
    "for c in cat_cols:\n",
    "    print(c, df[c].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fe1a38-c192-442f-9e0f-2d1cbfdc1c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_card_cols = [c for c in cat_cols if df[c].nunique() > 50]  # threshold can be tuned\n",
    "print(\"Dropping high-cardinality cols:\", high_card_cols)\n",
    "\n",
    "cat_cols = [c for c in cat_cols if c not in high_card_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd215f9d-0912-49cd-9fcf-1ff392a41d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_params = {'handle_unknown': 'ignore'}\n",
    "\n",
    "import sklearn\n",
    "if sklearn.__version__ >= \"1.2\":\n",
    "    ohe_params['sparse_output'] = True   # ✅ keep sparse\n",
    "else:\n",
    "    ohe_params['sparse'] = True\n",
    "\n",
    "cat_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(**ohe_params))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088f4cce-5cf3-4263-95b6-6d34b56eb3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df columns:\", df.columns.tolist())\n",
    "print(\"num_cols:\", num_cols)\n",
    "print(\"cat_cols:\", cat_cols)\n",
    "missing = [c for c in (num_cols + cat_cols) if c not in df.columns]\n",
    "print(\"Missing from df:\", missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669ab0c4-c2b4-495e-892d-0ca8a1499c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [c for c in num_cols if c in df.columns]\n",
    "cat_cols = [c for c in cat_cols if c in df.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae35f877-4a88-4982-8175-0bfcc49add26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winsorization helper\n",
    "def winsorize_array(X):\n",
    "    X = np.array(X, dtype=float)\n",
    "    lower = np.nanpercentile(X, 1, axis=0)\n",
    "    upper = np.nanpercentile(X, 99, axis=0)\n",
    "    return np.minimum(np.maximum(X, lower), upper)\n",
    "\n",
    "# Numeric pipeline\n",
    "num_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('winsor', FunctionTransformer(winsorize_array, validate=False)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Handle sklearn version\n",
    "ohe_params = {'handle_unknown': 'ignore'}\n",
    "if sklearn.__version__ >= \"1.2\":\n",
    "    ohe_params['sparse_output'] = True\n",
    "else:\n",
    "    ohe_params['sparse'] = True\n",
    "\n",
    "# Categorical pipeline\n",
    "cat_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(**ohe_params))\n",
    "])\n",
    "\n",
    "# Column transformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_transformer, num_cols),\n",
    "    ('cat', cat_transformer, cat_cols)\n",
    "], remainder='drop', verbose_feature_names_out=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f46dfe-209c-4c29-b258-19373ab2f66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = preprocessor.fit_transform(df)\n",
    "y_all = df['target_default'].values\n",
    "\n",
    "print(type(X_all))\n",
    "print(\"Preprocessing done. Shape:\", X_all.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486eb306-5209-4c08-9981-e8f756d00fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91e9ada-7bec-4ae7-a5bd-c7aa063c85d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8d0eec-f663-418b-9af5-cc3fdf4d613b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b7319c-9285-4c0b-be21-822be7a2e4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(preprocessor, 'preprocessor.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432f5aa1-8260-4e55-ba02-2d04db4262ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, df_train, df_test = train_test_split(\n",
    "    X_all, y_all, df, test_size=0.2, random_state=RANDOM_SEED, stratify=y_all\n",
    ")\n",
    "print(\"Train/Test shapes:\", X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1a1820-44f7-46d3-adac-6fccaf6ba299",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(\n",
    "    n_estimators=300, learning_rate=0.05, max_depth=6,\n",
    "    subsample=0.8, colsample_bytree=0.8, random_state=RANDOM_SEED,\n",
    "    use_label_encoder=False, eval_metric='logloss'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5302265f-1db7-42fe-bc7c-3b0822f5af59",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit(X_train, y_train)\n",
    "y_proba_xgb = xgb.predict_proba(X_test)[:,1]\n",
    "y_pred_xgb = (y_proba_xgb >= 0.5).astype(int)\n",
    "print(\"XGBoost AUC:\", roc_auc_score(y_test, y_proba_xgb))\n",
    "print(classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768cf886-00fe-4b21-80c5-a5610cdd44e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    feat_names = preprocessor.get_feature_names_out()\n",
    "    fi = pd.Series(xgb.feature_importances_, index=feat_names).sort_values(ascending=False).head(20)\n",
    "    print(\"Top features:\\n\", fi)\n",
    "    plt.figure(figsize=(8,6)); sns.barplot(x=fi.values, y=fi.index); plt.title(\"XGBoost feature importance (top20)\"); plt.show()\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6f8701-b017-4e72-8c74-f53494ce0c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "ann = keras.Sequential([\n",
    "    keras.layers.Input(shape=(input_dim,)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9983de-e6ca-4a8e-96be-7c73f1051d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', keras.metrics.AUC(name='auc')])\n",
    "history = ann.fit(X_train, y_train, validation_split=0.15, epochs=15, batch_size=256, verbose=1)\n",
    "ann_eval = ann.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"ANN test acc, auc:\", ann_eval[1], ann_eval[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f643b3-8990-40ab-b8a3-3eb37a84d50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1); plt.plot(history.history['loss'], label='train_loss'); plt.plot(history.history['val_loss'], label='val_loss'); plt.legend(); plt.title('Loss')\n",
    "plt.subplot(1,2,2); plt.plot(history.history['auc'], label='train_auc'); plt.plot(history.history['val_auc'], label='val_auc'); plt.legend(); plt.title('AUC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef4704f-3c72-4753-8626-724de4ad2036",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "Xtr = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "ytr = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "Xte = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "yte = torch.tensor(y_test, dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64636149-9381-4ebd-8070-dcb86d28d31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim,128), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(128,64), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(64,1)\n",
    "        )\n",
    "    def forward(self,x): return self.net(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90adb6e-a362-44fa-9b79-c9ed4636e9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(X_train.shape[1]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "loader = DataLoader(TensorDataset(Xtr,ytr), batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b6ab95-b570-4349-bce6-828830259ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(8):\n",
    "    model.train()\n",
    "    total=0\n",
    "    for xb,yb in loader:\n",
    "        opt.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total += loss.item()*xb.size(0)\n",
    "    print(f'Epoch {epoch+1} avg loss: {total/len(Xtr):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f93e0a-5e2b-41f8-8b9e-55146bc58f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(Xte).cpu().numpy()\n",
    "    probs = 1/(1+np.exp(-logits))\n",
    "print('PyTorch MLP AUC:', roc_auc_score(y_test, probs.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a305672-fc5c-44e2-bde6-e8bb88feb05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(preprocessor, 'preprocessor.joblib')\n",
    "xgb.save_model('xgb_model.json')\n",
    "ann.save('keras_ann.h5')\n",
    "torch.save(model.state_dict(), 'pytorch_mlp.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ccfd84-53cf-4eb9-854d-cf2d0651a098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65cb02-960d-489f-a8a5-ce4774b668a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8a5ec5-d3d3-4f1e-ae55-26cac3fa21cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711f583b-a79b-42fa-ab2e-01ac84ea075c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0244277e-95d1-46f2-a3e7-51499d3ea025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006bf588-43c6-4457-a47c-c768469a0b23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bca7fc6-9710-43b3-a3d3-2efa43273693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a281434-b3bc-4420-ad86-8edf7cdc5569",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
